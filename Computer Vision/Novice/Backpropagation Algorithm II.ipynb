{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm has two phases\n",
    "<!--  --> The Forward pass where inputs are passed through the network and output prediction obtained\n",
    "<!--  --> The Backward pass where we compute the gradient of the loss function at the final layer and use\n",
    "<!--  --> this gradient to recursively apply the chain rule to update the weights in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, alpha=0.1):\n",
    "        self.W = []\n",
    "        self.alpha = alpha\n",
    "        self.layers = layers\n",
    "        \n",
    "        # Weight initialization for  set of weight lines 1\n",
    "        for i in np.arange(0, len(layers) - 2):\n",
    "            w = np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n",
    "            self.W.append(w / np.sqrt(layers[i]))\n",
    "        \n",
    "        w = np.random.randn(layers[-2] + 1, layers[-1])\n",
    "        self.W.append(w /np.sqrt(layers[-2]))\n",
    "        \n",
    "    # Python magic function to format string\n",
    "    def __repr__(self):\n",
    "        return \"Neural Network: {}\".format(\"-\".join(str(l) for l in self.layers))\n",
    "    \n",
    "    # Sigmoid activation function\n",
    "    def sigmoid(self, x):\n",
    "        return 1. / (1 + np.exp(x))\n",
    "    \n",
    "    # In backpropagation when you choose activation function make sure to choose the one which is differentiable\n",
    "    # Below is a derivative of sigmoid function\n",
    "    def sigmoid_deriv(self, x):\n",
    "        return x * ( 1 - x)\n",
    "    \n",
    "    # Fit method for actual learning\n",
    "    def fit(self, X, y, epochs=1000, displayUpdate=100):\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    "        \n",
    "        # Train for a desired number of epochs\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            for (x, target) in zip(X, y):\n",
    "                self.partial_fit(x, target)\n",
    "        \n",
    "        # Display a training update\n",
    "        if epoch == 0 or (epoch + 1) % displayUpdate == 0:\n",
    "            loss = self.calculate_loss(X, y)\n",
    "            print(\"[INFO] epoch={}, loss={}\".format(str(epoch + 1), str(loss)))\n",
    "        \n",
    "    # partial_fit method\n",
    "    def partial_fit(self, x, y):\n",
    "        # Construct a list of activations\n",
    "        #The first activation is a special case -- its just the input feature vector itself\n",
    "        A = [np.atleast_2d(x)]\n",
    "        \n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # Feed forward the activation at the current layer by taking the dot product btn the activation and weight matrix\n",
    "            # This is called the net input to the current layer\n",
    "            net = A[layer].dot(self.W[layer])\n",
    "            \n",
    "            # Compute the net output by pass the net input to the activation function in our case is sigmoid\n",
    "            out = self.sigmoid(net)\n",
    "            \n",
    "            # Append the output to our activation list\n",
    "            A.append(out)\n",
    "            \n",
    "        #print(A)\n",
    "        # Backpropagation Phase\n",
    "        # The first step in backpropagation phase is to get the error\n",
    "        error = A[-1] - y\n",
    "        \n",
    "        # We have taken A[-1] since this is our last node in our network with a prediction\n",
    "        # From here we need to apply the chain rule and build our list of deltas D\n",
    "        # The first entry in our delta list is simply the error of the input layer times the derivative of our activation function for the output value\n",
    "        D = [error * self.sigmoid_deriv(A[-1])]\n",
    "        \n",
    "        # Chain rule application\n",
    "        for layer in np.arange(len(A) - 2, 0, -1):\n",
    "            # Delta of the current layer is equal to the delta of the previous layer dotted with the weight matrix of the current layer\n",
    "            # followed by the multiplying the delta by the derivetive of nonlinear activation function for the activation of the current later\n",
    "            delta = D[-1].dot(self.W[layer].T)\n",
    "            delta = delta * self.sigmoid_deriv(A[layer])\n",
    "            D.append(delta)\n",
    "        \n",
    "        #Reverse the deltas\n",
    "        D = D[::-1]\n",
    "        # print(D)\n",
    "        \n",
    "        # Weight update\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # Update weight by taking he dot product of the layer activations with their respective deltas, then multiplying this value\n",
    "            # by some small learning rate and adding to our weight matrix: this is where the actual learning takes place\n",
    "            self.W[layer] = -self.alpha * A[layer].T.dot(D[layer])\n",
    "        \n",
    "        #print(self.W) # This mark the end of backpropagation phase\n",
    "        \n",
    "    # Predict method to give out predictions\n",
    "    def predict(self, X, addBias=False):\n",
    "        # While prediction initialize the output prediction as the input features, to help with forward pass to obtain the final prediction\n",
    "        p = np.atleast_2d(X)\n",
    "        \n",
    "        # Check to see if we should add a bias column\n",
    "        if addBias:\n",
    "            # insert a columns of 1's to the input feature to reflect the bias trick\n",
    "            p = np.c_[p, np.ones((X.shape[0]))]\n",
    "            \n",
    "            # Loop over the network layers\n",
    "            for layer in np.arange(0, len(self.W)):\n",
    "                # compute the prediction by taking the dot product btn the current activation value p and the weight matrix associated \n",
    "                # with the current layer, then pass this value through a nonlinear activation function (sigmoid function)\n",
    "                p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "        \n",
    "        # return the predicted value\n",
    "        return p\n",
    "        \n",
    "    def calculate_loss(self, X, targets):\n",
    "        # Make predictions for the input data points then compute the loss\n",
    "        targets = np.atleast_2d(targets)\n",
    "        predictions = self.predict(X, addBias=False)\n",
    "        loss = 0.5 * (predictions - targets) ** 2\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.19681089, -0.54812268,  0.95172947],\n",
       "        [-1.17212331, -0.38055493, -0.01901262],\n",
       "        [ 0.18420064,  0.22515644, -0.25415963]]),\n",
       " array([[-2.03208034],\n",
       "        [ 0.09260364],\n",
       "        [ 0.53151717]])]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights initialized\n",
    "nn= NeuralNetwork([2,2,1])\n",
    "nn.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = nn.W\n",
    "len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network: 2-2-1\n"
     ]
    }
   ],
   "source": [
    "# Network architecture\n",
    "print(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray([[0,0], [0,1], [1,0], [1,1]])\n",
    "y = np.asarray([[0], [1], [1], [0]])\n",
    "# nn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch=2000, loss=[[0.  0.  0.5]\n",
      " [0.5 0.  0. ]\n",
      " [0.  0.5 0. ]\n",
      " [0.5 0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "nn2 = NeuralNetwork([2,2,1], alpha=0.5)\n",
    "nn2.fit(X, y, epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data=[0 0], ground_truth=0, pred=0.0000, step=0\n",
      "[INFO] data=[0 1], ground_truth=1, pred=0.0000, step=0\n",
      "[INFO] data=[1 0], ground_truth=1, pred=1.0000, step=1\n",
      "[INFO] data=[1 1], ground_truth=0, pred=1.0000, step=1\n"
     ]
    }
   ],
   "source": [
    "#Now that our network is trained we can loop over the XOR data points\n",
    "for (x, target) in zip(X, y):\n",
    "    # make prediction and display results\n",
    "    pred = nn2.predict(x)[0][0]\n",
    "    step = 1 if pred > 0.5 else 0\n",
    "    print(\"[INFO] data={}, ground_truth={}, pred={:.4f}, step={}\".format(x, target[0], pred, step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
