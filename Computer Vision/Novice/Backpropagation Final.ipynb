{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, alpha=0.1):\n",
    "        # Initialise the list of weight matrices, network architecture and learning rate\n",
    "        self.W = []\n",
    "        self.layers = layers\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Start looping from the index of the first layer but stop before we reach the last 2 layers\n",
    "        for i in np.arange(0, len(layers) - 2):\n",
    "            # Randomy initialise a weight matrix connecting the number of nodes in each respective layer together,\n",
    "            # adding an extra node for the bias\n",
    "            w = np.random.randn(layers[i] + 1, layers[i+1] + 1)\n",
    "            self.W.append(w / np.sqrt(layers[i]))\n",
    "\n",
    "        # The last 2 layers are a special case where the input connections need a bias term but the output does not\n",
    "        w = np.random.randn(layers[-2] + 1, layers[-1])\n",
    "        self.W.append(w / np.sqrt(layers[-2]))\n",
    "\n",
    "    def __repr__(self):\n",
    "        # Return string that represents the network architecture\n",
    "        return 'Neural Network: {}'.format('-'.join(str(l) for l in self.layers))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        # Compute the sigmoid activation value\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_deriv(self, x):\n",
    "        # Compute the derivative of the sigmoid function assuming that 'x' has already been passed through the\n",
    "        # sigmoid function\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def fit(self, X, y, epochs=1000, display_update=100):\n",
    "        # Insert a column of 1's as the last entry of the feature matrix. This allows us the treat the bias as a\n",
    "        # trainable parameter with the weight matrix\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    "\n",
    "        # Loop over the number of epochs\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            # Loop over each data point and train the network on it\n",
    "            for (x, target) in zip(X, y):\n",
    "                self.fit_partial(x, target)\n",
    "\n",
    "            # Check to see if we should display a training update\n",
    "            if epoch == 0 or (epoch + 1) % display_update == 0:\n",
    "                loss = self.calculate_loss(X, y)\n",
    "                print('[INFO]: epoch={}, loss={:.5f}'.format(epoch+1, loss))\n",
    "\n",
    "    def fit_partial(self, x, y):\n",
    "        # Construct list of output activities for each layer as the data point flows through the network. The first\n",
    "        # layer is just the input feature vector itself\n",
    "        A = [np.atleast_2d(x)]\n",
    "\n",
    "        # FEED-FORWARD:\n",
    "        # loop over the layers in the network\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # Feed forward the activation at the current layer by taking the dot product of the activation and the\n",
    "            # weight matrix - called the 'net input' to the current layer\n",
    "            net = A[layer].dot(self.W[layer])\n",
    "\n",
    "            # The 'net output' is simply applying the sigmoid function to the net input\n",
    "            out = self.sigmoid(net)\n",
    "\n",
    "            # Add the net output to the list of activations\n",
    "            A.append(out)\n",
    "\n",
    "        # BACK-PROPAGATION:\n",
    "        # Compute the difference between the 'prediction' (final net output in the activation list) and the true\n",
    "        # target value\n",
    "        error = A[-1] - y\n",
    "\n",
    "        # Apply the chain rule to build a list of deltas. The first entry is simply the error of the output layer\n",
    "        # times the derivative of the activation function for the ouput value\n",
    "        D = [error * self.sigmoid_deriv(A[-1])]\n",
    "\n",
    "        # Loop over the layers in reverse order (ignoring the last 2 layers)\n",
    "        for layer in np.arange(len(A) - 2, 0, -1):\n",
    "            # The delta for the current layer is equal to the delta of the 'previous layers' dotted with the weight\n",
    "            # matrix of the current layer, followed by multiplying the delta by the derivative of the activation\n",
    "            # function for the activations of the current layer\n",
    "            delta = D[-1].dot(self.W[layer].T)\n",
    "            delta = delta * self.sigmoid_deriv(A[layer])\n",
    "            D.append(delta)\n",
    "\n",
    "        # Since we looped over the layer in reverse order we need to reverse the deltas\n",
    "        D = D[::-1]\n",
    "\n",
    "        # WEIGHT-UPDATE-PHASE:\n",
    "        # Loop over the layers\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # Update the weights by taking the dot product of the layer activations with their respective deltas,\n",
    "            # then multiplying this value by the learning rate and adding to the weight matrix\n",
    "            self.W[layer] += -self.alpha * A[layer].T.dot(D[layer])\n",
    "\n",
    "    def predict(self, X, add_bias=True):\n",
    "        # Initialise the output prediction as the input features. This value will be (forward) propagated through the\n",
    "        # network to obtain the final prediction\n",
    "        p = np.atleast_2d(X)\n",
    "\n",
    "        # Check to see if the bias column should be added\n",
    "        if add_bias:\n",
    "            # Insert a column of 1's as the last entry in the feature matrix\n",
    "            p = np.c_[p, np.ones((p.shape[0]))]\n",
    "\n",
    "        # Loop over the layers in the network\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # Compute the output prediction\n",
    "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "\n",
    "        # Return the predicted value\n",
    "        return p\n",
    "\n",
    "    def calculate_loss(self, X, targets):\n",
    "        # Make predictions for the input data points then compute the loss\n",
    "        targets = np.atleast_2d(targets)\n",
    "        predictions = self.predict(X, add_bias=False)\n",
    "        loss = 0.5 * np.sum((predictions - targets) ** 2)\n",
    "\n",
    "        # Return the loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Training....\n",
      "[INFO]: epoch=1, loss=0.51339\n",
      "[INFO]: epoch=100, loss=0.50073\n",
      "[INFO]: epoch=200, loss=0.49913\n",
      "[INFO]: epoch=300, loss=0.49506\n",
      "[INFO]: epoch=400, loss=0.47824\n",
      "[INFO]: epoch=500, loss=0.43248\n",
      "[INFO]: epoch=600, loss=0.36512\n",
      "[INFO]: epoch=700, loss=0.29888\n",
      "[INFO]: epoch=800, loss=0.24597\n",
      "[INFO]: epoch=900, loss=0.20864\n",
      "[INFO]: epoch=1000, loss=0.18482\n",
      "[INFO]: epoch=1100, loss=0.16982\n",
      "[INFO]: epoch=1200, loss=0.16003\n",
      "[INFO]: epoch=1300, loss=0.15334\n",
      "[INFO]: epoch=1400, loss=0.14856\n",
      "[INFO]: epoch=1500, loss=0.14502\n",
      "[INFO]: epoch=1600, loss=0.14231\n",
      "[INFO]: epoch=1700, loss=0.14017\n",
      "[INFO]: epoch=1800, loss=0.13846\n",
      "[INFO]: epoch=1900, loss=0.13706\n",
      "[INFO]: epoch=2000, loss=0.13589\n",
      "[INFO]: epoch=2100, loss=0.13490\n",
      "[INFO]: epoch=2200, loss=0.13405\n",
      "[INFO]: epoch=2300, loss=0.13332\n",
      "[INFO]: epoch=2400, loss=0.13268\n",
      "[INFO]: epoch=2500, loss=0.13211\n",
      "[INFO]: epoch=2600, loss=0.13161\n",
      "[INFO]: epoch=2700, loss=0.13115\n",
      "[INFO]: epoch=2800, loss=0.13074\n",
      "[INFO]: epoch=2900, loss=0.13036\n",
      "[INFO]: epoch=3000, loss=0.13002\n",
      "[INFO]: epoch=3100, loss=0.12970\n",
      "[INFO]: epoch=3200, loss=0.12939\n",
      "[INFO]: epoch=3300, loss=0.12911\n",
      "[INFO]: epoch=3400, loss=0.12884\n",
      "[INFO]: epoch=3500, loss=0.12857\n",
      "[INFO]: epoch=3600, loss=0.12831\n",
      "[INFO]: epoch=3700, loss=0.12805\n",
      "[INFO]: epoch=3800, loss=0.12779\n",
      "[INFO]: epoch=3900, loss=0.12751\n",
      "[INFO]: epoch=4000, loss=0.12722\n",
      "[INFO]: epoch=4100, loss=0.12689\n",
      "[INFO]: epoch=4200, loss=0.12652\n",
      "[INFO]: epoch=4300, loss=0.12607\n",
      "[INFO]: epoch=4400, loss=0.12551\n",
      "[INFO]: epoch=4500, loss=0.12475\n",
      "[INFO]: epoch=4600, loss=0.12362\n",
      "[INFO]: epoch=4700, loss=0.12175\n",
      "[INFO]: epoch=4800, loss=0.11803\n",
      "[INFO]: epoch=4900, loss=0.10819\n",
      "[INFO]: epoch=5000, loss=0.07426\n",
      "[INFO]: epoch=5100, loss=0.04675\n",
      "[INFO]: epoch=5200, loss=0.03623\n",
      "[INFO]: epoch=5300, loss=0.02939\n",
      "[INFO]: epoch=5400, loss=0.02452\n",
      "[INFO]: epoch=5500, loss=0.02088\n",
      "[INFO]: epoch=5600, loss=0.01804\n",
      "[INFO]: epoch=5700, loss=0.01578\n",
      "[INFO]: epoch=5800, loss=0.01394\n",
      "[INFO]: epoch=5900, loss=0.01241\n",
      "[INFO]: epoch=6000, loss=0.01113\n",
      "[INFO]: epoch=6100, loss=0.01004\n",
      "[INFO]: epoch=6200, loss=0.00910\n",
      "[INFO]: epoch=6300, loss=0.00829\n",
      "[INFO]: epoch=6400, loss=0.00758\n",
      "[INFO]: epoch=6500, loss=0.00696\n",
      "[INFO]: epoch=6600, loss=0.00641\n",
      "[INFO]: epoch=6700, loss=0.00593\n",
      "[INFO]: epoch=6800, loss=0.00550\n",
      "[INFO]: epoch=6900, loss=0.00511\n",
      "[INFO]: epoch=7000, loss=0.00476\n",
      "[INFO]: epoch=7100, loss=0.00445\n",
      "[INFO]: epoch=7200, loss=0.00417\n",
      "[INFO]: epoch=7300, loss=0.00391\n",
      "[INFO]: epoch=7400, loss=0.00368\n",
      "[INFO]: epoch=7500, loss=0.00347\n",
      "[INFO]: epoch=7600, loss=0.00328\n",
      "[INFO]: epoch=7700, loss=0.00310\n",
      "[INFO]: epoch=7800, loss=0.00294\n",
      "[INFO]: epoch=7900, loss=0.00279\n",
      "[INFO]: epoch=8000, loss=0.00265\n",
      "[INFO]: epoch=8100, loss=0.00252\n",
      "[INFO]: epoch=8200, loss=0.00241\n",
      "[INFO]: epoch=8300, loss=0.00230\n",
      "[INFO]: epoch=8400, loss=0.00220\n",
      "[INFO]: epoch=8500, loss=0.00210\n",
      "[INFO]: epoch=8600, loss=0.00202\n",
      "[INFO]: epoch=8700, loss=0.00194\n",
      "[INFO]: epoch=8800, loss=0.00186\n",
      "[INFO]: epoch=8900, loss=0.00179\n",
      "[INFO]: epoch=9000, loss=0.00172\n",
      "[INFO]: epoch=9100, loss=0.00166\n",
      "[INFO]: epoch=9200, loss=0.00160\n",
      "[INFO]: epoch=9300, loss=0.00155\n",
      "[INFO]: epoch=9400, loss=0.00149\n",
      "[INFO]: epoch=9500, loss=0.00145\n",
      "[INFO]: epoch=9600, loss=0.00140\n",
      "[INFO]: epoch=9700, loss=0.00135\n",
      "[INFO]: epoch=9800, loss=0.00131\n",
      "[INFO]: epoch=9900, loss=0.00127\n",
      "[INFO]: epoch=10000, loss=0.00124\n",
      "[INFO]: epoch=10100, loss=0.00120\n",
      "[INFO]: epoch=10200, loss=0.00117\n",
      "[INFO]: epoch=10300, loss=0.00114\n",
      "[INFO]: epoch=10400, loss=0.00110\n",
      "[INFO]: epoch=10500, loss=0.00108\n",
      "[INFO]: epoch=10600, loss=0.00105\n",
      "[INFO]: epoch=10700, loss=0.00102\n",
      "[INFO]: epoch=10800, loss=0.00100\n",
      "[INFO]: epoch=10900, loss=0.00097\n",
      "[INFO]: epoch=11000, loss=0.00095\n",
      "[INFO]: epoch=11100, loss=0.00093\n",
      "[INFO]: epoch=11200, loss=0.00090\n",
      "[INFO]: epoch=11300, loss=0.00088\n",
      "[INFO]: epoch=11400, loss=0.00086\n",
      "[INFO]: epoch=11500, loss=0.00085\n",
      "[INFO]: epoch=11600, loss=0.00083\n",
      "[INFO]: epoch=11700, loss=0.00081\n",
      "[INFO]: epoch=11800, loss=0.00079\n",
      "[INFO]: epoch=11900, loss=0.00078\n",
      "[INFO]: epoch=12000, loss=0.00076\n",
      "[INFO]: epoch=12100, loss=0.00075\n",
      "[INFO]: epoch=12200, loss=0.00073\n",
      "[INFO]: epoch=12300, loss=0.00072\n",
      "[INFO]: epoch=12400, loss=0.00070\n",
      "[INFO]: epoch=12500, loss=0.00069\n",
      "[INFO]: epoch=12600, loss=0.00068\n",
      "[INFO]: epoch=12700, loss=0.00067\n",
      "[INFO]: epoch=12800, loss=0.00065\n",
      "[INFO]: epoch=12900, loss=0.00064\n",
      "[INFO]: epoch=13000, loss=0.00063\n",
      "[INFO]: epoch=13100, loss=0.00062\n",
      "[INFO]: epoch=13200, loss=0.00061\n",
      "[INFO]: epoch=13300, loss=0.00060\n",
      "[INFO]: epoch=13400, loss=0.00059\n",
      "[INFO]: epoch=13500, loss=0.00058\n",
      "[INFO]: epoch=13600, loss=0.00057\n",
      "[INFO]: epoch=13700, loss=0.00056\n",
      "[INFO]: epoch=13800, loss=0.00055\n",
      "[INFO]: epoch=13900, loss=0.00055\n",
      "[INFO]: epoch=14000, loss=0.00054\n",
      "[INFO]: epoch=14100, loss=0.00053\n",
      "[INFO]: epoch=14200, loss=0.00052\n",
      "[INFO]: epoch=14300, loss=0.00051\n",
      "[INFO]: epoch=14400, loss=0.00051\n",
      "[INFO]: epoch=14500, loss=0.00050\n",
      "[INFO]: epoch=14600, loss=0.00049\n",
      "[INFO]: epoch=14700, loss=0.00049\n",
      "[INFO]: epoch=14800, loss=0.00048\n",
      "[INFO]: epoch=14900, loss=0.00047\n",
      "[INFO]: epoch=15000, loss=0.00047\n",
      "[INFO]: epoch=15100, loss=0.00046\n",
      "[INFO]: epoch=15200, loss=0.00045\n",
      "[INFO]: epoch=15300, loss=0.00045\n",
      "[INFO]: epoch=15400, loss=0.00044\n",
      "[INFO]: epoch=15500, loss=0.00044\n",
      "[INFO]: epoch=15600, loss=0.00043\n",
      "[INFO]: epoch=15700, loss=0.00043\n",
      "[INFO]: epoch=15800, loss=0.00042\n",
      "[INFO]: epoch=15900, loss=0.00042\n",
      "[INFO]: epoch=16000, loss=0.00041\n",
      "[INFO]: epoch=16100, loss=0.00041\n",
      "[INFO]: epoch=16200, loss=0.00040\n",
      "[INFO]: epoch=16300, loss=0.00040\n",
      "[INFO]: epoch=16400, loss=0.00039\n",
      "[INFO]: epoch=16500, loss=0.00039\n",
      "[INFO]: epoch=16600, loss=0.00038\n",
      "[INFO]: epoch=16700, loss=0.00038\n",
      "[INFO]: epoch=16800, loss=0.00038\n",
      "[INFO]: epoch=16900, loss=0.00037\n",
      "[INFO]: epoch=17000, loss=0.00037\n",
      "[INFO]: epoch=17100, loss=0.00036\n",
      "[INFO]: epoch=17200, loss=0.00036\n",
      "[INFO]: epoch=17300, loss=0.00036\n",
      "[INFO]: epoch=17400, loss=0.00035\n",
      "[INFO]: epoch=17500, loss=0.00035\n",
      "[INFO]: epoch=17600, loss=0.00034\n",
      "[INFO]: epoch=17700, loss=0.00034\n",
      "[INFO]: epoch=17800, loss=0.00034\n",
      "[INFO]: epoch=17900, loss=0.00033\n",
      "[INFO]: epoch=18000, loss=0.00033\n",
      "[INFO]: epoch=18100, loss=0.00033\n",
      "[INFO]: epoch=18200, loss=0.00032\n",
      "[INFO]: epoch=18300, loss=0.00032\n",
      "[INFO]: epoch=18400, loss=0.00032\n",
      "[INFO]: epoch=18500, loss=0.00032\n",
      "[INFO]: epoch=18600, loss=0.00031\n",
      "[INFO]: epoch=18700, loss=0.00031\n",
      "[INFO]: epoch=18800, loss=0.00031\n",
      "[INFO]: epoch=18900, loss=0.00030\n",
      "[INFO]: epoch=19000, loss=0.00030\n",
      "[INFO]: epoch=19100, loss=0.00030\n",
      "[INFO]: epoch=19200, loss=0.00030\n",
      "[INFO]: epoch=19300, loss=0.00029\n",
      "[INFO]: epoch=19400, loss=0.00029\n",
      "[INFO]: epoch=19500, loss=0.00029\n",
      "[INFO]: epoch=19600, loss=0.00029\n",
      "[INFO]: epoch=19700, loss=0.00028\n",
      "[INFO]: epoch=19800, loss=0.00028\n",
      "[INFO]: epoch=19900, loss=0.00028\n",
      "[INFO]: epoch=20000, loss=0.00028\n",
      "[INFO]: Testing....\n",
      "[INFO]: Data=[0 0], Ground Truth=0, Prediction=0.0095, Step=0\n",
      "[INFO]: Data=[0 1], Ground Truth=1, Prediction=0.9917, Step=1\n",
      "[INFO]: Data=[1 0], Ground Truth=1, Prediction=0.9862, Step=1\n",
      "[INFO]: Data=[1 1], Ground Truth=0, Prediction=0.0143, Step=0\n"
     ]
    }
   ],
   "source": [
    "# Construct the 'XOR' dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Train the NN\n",
    "print('[INFO]: Training....')\n",
    "nn = NeuralNetwork([2, 2, 1], alpha=0.5)\n",
    "nn.fit(X, y, epochs=20000)\n",
    "\n",
    "# Test the NN\n",
    "print('[INFO]: Testing....')\n",
    "\n",
    "# Loop over the data points\n",
    "for (x, target) in zip(X, y):\n",
    "    # Make a prediction and display the result\n",
    "    pred = nn.predict(x)[0][0]\n",
    "    step = 1 if pred > 0.5 else 0\n",
    "    print('[INFO]: Data={}, Ground Truth={}, Prediction={:.4f}, Step={}'.format(x, target[0], pred, step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
