{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Implementation with backpropagation\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, alpha=0.1):\n",
    "        self.W = []\n",
    "        self.alpha = alpha\n",
    "        self.layers = layers\n",
    "        \n",
    "        # Weights initialization\n",
    "        # Before last two layers\n",
    "        for i in np.arange(0, len(layers) - 2):\n",
    "            # initialize weights by including a bias trick\n",
    "            w = np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n",
    "            self.W.append(w / np.sqrt(layers[i]))\n",
    "        \n",
    "        # Between last two layers\n",
    "        w = np.random.randn(layers[-2] + 1, layers[-1])\n",
    "        self.W.append(w / np.sqrt(layers[-2]))\n",
    "        \n",
    "    # This method allow us to print the network architecture\n",
    "    def __repr__(self):\n",
    "        return \"Neural Network: {} \".format(\"-\".join(str(l) for l in self.layers))\n",
    "    \n",
    "    # Define your activation function and its derivative\n",
    "    # Sigmoid activation function\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1 + np.exp(x))\n",
    "    \n",
    "    # Derivative of sigmoid function\n",
    "    def sigmoid_deriv(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    # Train a network using fit method\n",
    "    def fit(self, X, y, epochs = 1000, displayUpdate=100):\n",
    "        # Format the input features to comply with a bias trick by inserting a column of 1\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    "        \n",
    "        # Loop over the desired number of epochs and insert data into the network for learning\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            for (x, target) in zip(X, y):\n",
    "                self.partial_fit(x, target)\n",
    "            \n",
    "            # Display a training update by looking at loss and epochs\n",
    "            if epoch == 0 or (epoch + 1) % displayUpdate == 0:\n",
    "                loss = self.calculate_loss(X, y)\n",
    "                print(\"[INFO] epoch={}, loss = {}\". format(epoch + 1, loss))\n",
    "    \n",
    "    # This method clculates the activation output of each layer and store them in activation list\n",
    "    def partial_fit(self, x, y):\n",
    "        #FORWARD PASS\n",
    "        # Initialize activation list with the first entry which is the same as the input feature vector it self\n",
    "        A = [np.atleast_2d(x)]\n",
    "        \n",
    "        # Loop over the network and find the activations\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # calculate the net input by taking activation of the current layer times weight matrix\n",
    "            net = A[layer].dot(self.W[layer])\n",
    "            \n",
    "            # calculate net output by passing net input through sigmoid function\n",
    "            out = self.sigmoid(net)\n",
    "            \n",
    "            # Insert net out put to activation list\n",
    "            A.append(out)\n",
    "        \n",
    "        # BACKPROPAGATION\n",
    "        # First compute an error (The diference between our prediction and true target)\n",
    "        # Remember activation list hold predictions for each neuron and the last neuron (A[-1]) is the general prediction a network provide\n",
    "        net_pred = A[-1]\n",
    "        error = A[-1] - y\n",
    "        \n",
    "        # Apply a chain rule and build a list of deltas D\n",
    "        #The first entry of our delta is simply an error times the derivative of activation function for the output value\n",
    "        D = [ error * self.sigmoid_deriv(net_pred)]\n",
    "#         print(D)\n",
    "        \n",
    "        # To find other deltas simply loop over the layers in reverse order\n",
    "        for layer in np.arange(len(A) - 2, 0, -1):\n",
    "            # The delta for the currnt network is equal to the delta of the previous layer dotted with the weight matrix of the current layer\n",
    "            # Followed by multiplying delta by the derivative of our activation function for the activation of the current layer\n",
    "            delta = D[-1].dot(self.W[layer].T)\n",
    "            delta = delta * self.sigmoid_deriv(A[layer])\n",
    "            D.append(delta)\n",
    "        \n",
    "        # Reverse list of Deltas\n",
    "        reversed_deltas = D[::-1]\n",
    "        \n",
    "        # WEIGHT UPDATE\n",
    "        # loop over the layers and perform the update\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            self.W[layer] += -self.alpha * A[layer].T.dot(reversed_deltas[layer])\n",
    "#         print('Activation List from feedforward')\n",
    "#         print('')\n",
    "#         print(A)\n",
    "#         print('')\n",
    "#         print('List of deltas')        \n",
    "#         print(D)\n",
    "#         print('')\n",
    "#         print('Revesed Deltas')\n",
    "#         print('')\n",
    "#         print(reversed_deltas)\n",
    "#         print('')\n",
    "#         print('Updated weights')\n",
    "#         print('')\n",
    "#         print(self.W)\n",
    "\n",
    "      # Predict method to give out predictions\n",
    "    def predict(self, X, addBias=True):\n",
    "        # While prediction initialize the output prediction as the input features, to help with forward pass to obtain the final prediction\n",
    "        p = np.atleast_2d(X)\n",
    "        \n",
    "        # Check to see if we should add a bias column\n",
    "        if addBias:\n",
    "            # insert a columns of 1's to the input feature to reflect the bias trick\n",
    "            p = np.c_[p, np.ones((p.shape[0]))]\n",
    "            \n",
    "            # Loop over the network layers\n",
    "            for layer in np.arange(0, len(self.W)):\n",
    "                # compute the prediction by taking the dot product btn the current activation value p and the weight matrix associated \n",
    "                # with the current layer, then pass this value through a nonlinear activation function (sigmoid function)\n",
    "                p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "        \n",
    "        # return the predicted value\n",
    "        return p\n",
    "        \n",
    "    def calculate_loss(self, X, targets):\n",
    "        # Make predictions for the input data points then compute the loss\n",
    "        targets = np.atleast_2d(targets)\n",
    "        predictions = self.predict(X, addBias=False)\n",
    "        loss = 0.5 * np.sum((predictions - targets) ** 2)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Training....\n",
      "[INFO] epoch=1, loss = 3.0\n",
      "[INFO] epoch=100, loss = 3.0\n",
      "[INFO] epoch=200, loss = 3.0\n",
      "[INFO] epoch=300, loss = 3.0\n",
      "[INFO] epoch=400, loss = 3.0\n",
      "[INFO] epoch=500, loss = 3.0\n",
      "[INFO] epoch=600, loss = 3.0\n",
      "[INFO] epoch=700, loss = 3.0\n",
      "[INFO] epoch=800, loss = 3.0\n",
      "[INFO] epoch=900, loss = 3.0\n",
      "[INFO] epoch=1000, loss = 3.0\n",
      "[INFO] epoch=1100, loss = 3.0\n",
      "[INFO] epoch=1200, loss = 3.0\n",
      "[INFO] epoch=1300, loss = 3.0\n",
      "[INFO] epoch=1400, loss = 3.0\n",
      "[INFO] epoch=1500, loss = 3.0\n",
      "[INFO] epoch=1600, loss = 3.0\n",
      "[INFO] epoch=1700, loss = 3.0\n",
      "[INFO] epoch=1800, loss = 3.0\n",
      "[INFO] epoch=1900, loss = 3.0\n",
      "[INFO] epoch=2000, loss = 3.0\n",
      "[INFO] epoch=2100, loss = 3.0\n",
      "[INFO] epoch=2200, loss = 3.0\n",
      "[INFO] epoch=2300, loss = 3.0\n",
      "[INFO] epoch=2400, loss = 3.0\n",
      "[INFO] epoch=2500, loss = 3.0\n",
      "[INFO] epoch=2600, loss = 3.0\n",
      "[INFO] epoch=2700, loss = 3.0\n",
      "[INFO] epoch=2800, loss = 3.0\n",
      "[INFO] epoch=2900, loss = 3.0\n",
      "[INFO] epoch=3000, loss = 3.0\n",
      "[INFO] epoch=3100, loss = 3.0\n",
      "[INFO] epoch=3200, loss = 3.0\n",
      "[INFO] epoch=3300, loss = 3.0\n",
      "[INFO] epoch=3400, loss = 3.0\n",
      "[INFO] epoch=3500, loss = 3.0\n",
      "[INFO] epoch=3600, loss = 3.0\n",
      "[INFO] epoch=3700, loss = 3.0\n",
      "[INFO] epoch=3800, loss = 3.0\n",
      "[INFO] epoch=3900, loss = 3.0\n",
      "[INFO] epoch=4000, loss = 3.0\n",
      "[INFO] epoch=4100, loss = 3.0\n",
      "[INFO] epoch=4200, loss = 3.0\n",
      "[INFO] epoch=4300, loss = 3.0\n",
      "[INFO] epoch=4400, loss = 3.0\n",
      "[INFO] epoch=4500, loss = 3.0\n",
      "[INFO] epoch=4600, loss = 3.0\n",
      "[INFO] epoch=4700, loss = 3.0\n",
      "[INFO] epoch=4800, loss = 3.0\n",
      "[INFO] epoch=4900, loss = 3.0\n",
      "[INFO] epoch=5000, loss = 3.0\n",
      "[INFO] epoch=5100, loss = 3.0\n",
      "[INFO] epoch=5200, loss = 3.0\n",
      "[INFO] epoch=5300, loss = 3.0\n",
      "[INFO] epoch=5400, loss = 3.0\n",
      "[INFO] epoch=5500, loss = 3.0\n",
      "[INFO] epoch=5600, loss = 3.0\n",
      "[INFO] epoch=5700, loss = 3.0\n",
      "[INFO] epoch=5800, loss = 3.0\n",
      "[INFO] epoch=5900, loss = 3.0\n",
      "[INFO] epoch=6000, loss = 3.0\n",
      "[INFO] epoch=6100, loss = 3.0\n",
      "[INFO] epoch=6200, loss = 3.0\n",
      "[INFO] epoch=6300, loss = 3.0\n",
      "[INFO] epoch=6400, loss = 3.0\n",
      "[INFO] epoch=6500, loss = 3.0\n",
      "[INFO] epoch=6600, loss = 3.0\n",
      "[INFO] epoch=6700, loss = 3.0\n",
      "[INFO] epoch=6800, loss = 3.0\n",
      "[INFO] epoch=6900, loss = 3.0\n",
      "[INFO] epoch=7000, loss = 3.0\n",
      "[INFO] epoch=7100, loss = 3.0\n",
      "[INFO] epoch=7200, loss = 3.0\n",
      "[INFO] epoch=7300, loss = 3.0\n",
      "[INFO] epoch=7400, loss = 3.0\n",
      "[INFO] epoch=7500, loss = 3.0\n",
      "[INFO] epoch=7600, loss = 3.0\n",
      "[INFO] epoch=7700, loss = 3.0\n",
      "[INFO] epoch=7800, loss = 3.0\n",
      "[INFO] epoch=7900, loss = 3.0\n",
      "[INFO] epoch=8000, loss = 3.0\n",
      "[INFO] epoch=8100, loss = 3.0\n",
      "[INFO] epoch=8200, loss = 3.0\n",
      "[INFO] epoch=8300, loss = 3.0\n",
      "[INFO] epoch=8400, loss = 3.0\n",
      "[INFO] epoch=8500, loss = 3.0\n",
      "[INFO] epoch=8600, loss = 3.0\n",
      "[INFO] epoch=8700, loss = 3.0\n",
      "[INFO] epoch=8800, loss = 3.0\n",
      "[INFO] epoch=8900, loss = 3.0\n",
      "[INFO] epoch=9000, loss = 3.0\n",
      "[INFO] epoch=9100, loss = 3.0\n",
      "[INFO] epoch=9200, loss = 3.0\n",
      "[INFO] epoch=9300, loss = 3.0\n",
      "[INFO] epoch=9400, loss = 3.0\n",
      "[INFO] epoch=9500, loss = 3.0\n",
      "[INFO] epoch=9600, loss = 3.0\n",
      "[INFO] epoch=9700, loss = 3.0\n",
      "[INFO] epoch=9800, loss = 3.0\n",
      "[INFO] epoch=9900, loss = 3.0\n",
      "[INFO] epoch=10000, loss = 3.0\n",
      "[INFO] epoch=10100, loss = 3.0\n",
      "[INFO] epoch=10200, loss = 3.0\n",
      "[INFO] epoch=10300, loss = 3.0\n",
      "[INFO] epoch=10400, loss = 3.0\n",
      "[INFO] epoch=10500, loss = 3.0\n",
      "[INFO] epoch=10600, loss = 3.0\n",
      "[INFO] epoch=10700, loss = 3.0\n",
      "[INFO] epoch=10800, loss = 3.0\n",
      "[INFO] epoch=10900, loss = 3.0\n",
      "[INFO] epoch=11000, loss = 3.0\n",
      "[INFO] epoch=11100, loss = 3.0\n",
      "[INFO] epoch=11200, loss = 3.0\n",
      "[INFO] epoch=11300, loss = 3.0\n",
      "[INFO] epoch=11400, loss = 3.0\n",
      "[INFO] epoch=11500, loss = 3.0\n",
      "[INFO] epoch=11600, loss = 3.0\n",
      "[INFO] epoch=11700, loss = 3.0\n",
      "[INFO] epoch=11800, loss = 3.0\n",
      "[INFO] epoch=11900, loss = 3.0\n",
      "[INFO] epoch=12000, loss = 3.0\n",
      "[INFO] epoch=12100, loss = 3.0\n",
      "[INFO] epoch=12200, loss = 3.0\n",
      "[INFO] epoch=12300, loss = 3.0\n",
      "[INFO] epoch=12400, loss = 3.0\n",
      "[INFO] epoch=12500, loss = 3.0\n",
      "[INFO] epoch=12600, loss = 3.0\n",
      "[INFO] epoch=12700, loss = 3.0\n",
      "[INFO] epoch=12800, loss = 3.0\n",
      "[INFO] epoch=12900, loss = 3.0\n",
      "[INFO] epoch=13000, loss = 3.0\n",
      "[INFO] epoch=13100, loss = 3.0\n",
      "[INFO] epoch=13200, loss = 3.0\n",
      "[INFO] epoch=13300, loss = 3.0\n",
      "[INFO] epoch=13400, loss = 3.0\n",
      "[INFO] epoch=13500, loss = 3.0\n",
      "[INFO] epoch=13600, loss = 3.0\n",
      "[INFO] epoch=13700, loss = 3.0\n",
      "[INFO] epoch=13800, loss = 3.0\n",
      "[INFO] epoch=13900, loss = 3.0\n",
      "[INFO] epoch=14000, loss = 3.0\n",
      "[INFO] epoch=14100, loss = 3.0\n",
      "[INFO] epoch=14200, loss = 3.0\n",
      "[INFO] epoch=14300, loss = 3.0\n",
      "[INFO] epoch=14400, loss = 3.0\n",
      "[INFO] epoch=14500, loss = 3.0\n",
      "[INFO] epoch=14600, loss = 3.0\n",
      "[INFO] epoch=14700, loss = 3.0\n",
      "[INFO] epoch=14800, loss = 3.0\n",
      "[INFO] epoch=14900, loss = 3.0\n",
      "[INFO] epoch=15000, loss = 3.0\n",
      "[INFO] epoch=15100, loss = 3.0\n",
      "[INFO] epoch=15200, loss = 3.0\n",
      "[INFO] epoch=15300, loss = 3.0\n",
      "[INFO] epoch=15400, loss = 3.0\n",
      "[INFO] epoch=15500, loss = 3.0\n",
      "[INFO] epoch=15600, loss = 3.0\n",
      "[INFO] epoch=15700, loss = 3.0\n",
      "[INFO] epoch=15800, loss = 3.0\n",
      "[INFO] epoch=15900, loss = 3.0\n",
      "[INFO] epoch=16000, loss = 3.0\n",
      "[INFO] epoch=16100, loss = 3.0\n",
      "[INFO] epoch=16200, loss = 3.0\n",
      "[INFO] epoch=16300, loss = 3.0\n",
      "[INFO] epoch=16400, loss = 3.0\n",
      "[INFO] epoch=16500, loss = 3.0\n",
      "[INFO] epoch=16600, loss = 3.0\n",
      "[INFO] epoch=16700, loss = 3.0\n",
      "[INFO] epoch=16800, loss = 3.0\n",
      "[INFO] epoch=16900, loss = 3.0\n",
      "[INFO] epoch=17000, loss = 3.0\n",
      "[INFO] epoch=17100, loss = 3.0\n",
      "[INFO] epoch=17200, loss = 3.0\n",
      "[INFO] epoch=17300, loss = 3.0\n",
      "[INFO] epoch=17400, loss = 3.0\n",
      "[INFO] epoch=17500, loss = 3.0\n",
      "[INFO] epoch=17600, loss = 3.0\n",
      "[INFO] epoch=17700, loss = 3.0\n",
      "[INFO] epoch=17800, loss = 3.0\n",
      "[INFO] epoch=17900, loss = 3.0\n",
      "[INFO] epoch=18000, loss = 3.0\n",
      "[INFO] epoch=18100, loss = 3.0\n",
      "[INFO] epoch=18200, loss = 3.0\n",
      "[INFO] epoch=18300, loss = 3.0\n",
      "[INFO] epoch=18400, loss = 3.0\n",
      "[INFO] epoch=18500, loss = 3.0\n",
      "[INFO] epoch=18600, loss = 3.0\n",
      "[INFO] epoch=18700, loss = 3.0\n",
      "[INFO] epoch=18800, loss = 3.0\n",
      "[INFO] epoch=18900, loss = 3.0\n",
      "[INFO] epoch=19000, loss = 3.0\n",
      "[INFO] epoch=19100, loss = 3.0\n",
      "[INFO] epoch=19200, loss = 3.0\n",
      "[INFO] epoch=19300, loss = 3.0\n",
      "[INFO] epoch=19400, loss = 3.0\n",
      "[INFO] epoch=19500, loss = 3.0\n",
      "[INFO] epoch=19600, loss = 3.0\n",
      "[INFO] epoch=19700, loss = 3.0\n",
      "[INFO] epoch=19800, loss = 3.0\n",
      "[INFO] epoch=19900, loss = 3.0\n",
      "[INFO] epoch=20000, loss = 3.0\n",
      "[INFO]: Testing....\n",
      "[INFO]: Data=[0 0], Ground Truth=0, Prediction=0.5008, Step=1\n",
      "[INFO]: Data=[0 1], Ground Truth=1, Prediction=0.5001, Step=1\n",
      "[INFO]: Data=[1 0], Ground Truth=1, Prediction=0.5001, Step=1\n",
      "[INFO]: Data=[1 1], Ground Truth=0, Prediction=0.5000, Step=1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Construct the 'XOR' dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Train the NN\n",
    "print('[INFO]: Training....')\n",
    "nn = NeuralNetwork([2, 2, 1], alpha=0.5)\n",
    "nn.fit(X, y, epochs=20000)\n",
    "\n",
    "# Test the NN\n",
    "print('[INFO]: Testing....')\n",
    "\n",
    "# Loop over the data points\n",
    "for (x, target) in zip(X, y):\n",
    "    # Make a prediction and display the result\n",
    "    pred = nn.predict(x)[0][0]\n",
    "    step = 1 if pred > 0.5 else 0\n",
    "    print('[INFO]: Data={}, Ground Truth={}, Prediction={:.4f}, Step={}'.format(x, target[0], pred, step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
