{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from itertools import islice\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "def take(n, iterable):\n",
    "    # Return first item of the iterable as a list\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acqusition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tajr\\Anaconda3\\envs\\Deep_Learning\\lib\\site-packages\\keras\\datasets\\reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\Tajr\\Anaconda3\\envs\\Deep_Learning\\lib\\site-packages\\keras\\datasets\\reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8982"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
      "[1, 3267, 699, 3434, 2295, 56, 2, 7511, 9, 56, 3906, 1073, 81, 5, 1198, 57, 366, 737, 132, 20, 4093, 7, 2, 49, 2295, 2, 1037, 3267, 699, 3434, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2295, 2, 2, 775, 7, 48, 34, 191, 44, 35, 1795, 505, 17, 12]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 2):\n",
    "    print(train_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 1378, 2025, 9, 697, 4622, 111, 8, 25, 109, 29, 3650, 11, 150, 244, 364, 33, 30, 30, 1398, 333, 6, 2, 159, 9, 1084, 363, 13, 2, 71, 9, 2, 71, 117, 4, 225, 78, 206, 10, 9, 1214, 8, 4, 270, 5, 2, 7, 748, 48, 9, 2, 7, 207, 1451, 966, 1864, 793, 97, 133, 336, 7, 4, 493, 98, 273, 104, 284, 25, 39, 338, 22, 905, 220, 3465, 644, 59, 20, 6, 119, 61, 11, 15, 58, 579, 26, 10, 67, 7, 4, 738, 98, 43, 88, 333, 722, 12, 20, 6, 19, 746, 35, 15, 10, 9, 1214, 855, 129, 783, 21, 4, 2280, 244, 364, 51, 16, 299, 452, 16, 515, 4, 99, 29, 5, 4, 364, 281, 48, 10, 9, 1214, 23, 644, 47, 20, 324, 27, 56, 2, 2, 5, 192, 510, 17, 12]\n",
      "[1, 2768, 283, 122, 7, 4, 89, 544, 463, 29, 798, 748, 40, 85, 306, 28, 19, 59, 11, 82, 84, 22, 10, 1315, 19, 12, 11, 82, 52, 29, 283, 1135, 558, 2, 265, 2, 6607, 8, 6607, 118, 371, 10, 1503, 281, 4, 143, 4811, 760, 50, 2088, 225, 139, 683, 4, 48, 193, 862, 41, 967, 1999, 30, 1086, 36, 8, 28, 602, 19, 32, 11, 82, 5, 4, 89, 544, 463, 41, 30, 6273, 13, 260, 951, 6607, 8, 69, 1749, 18, 82, 41, 30, 306, 3342, 13, 4, 37, 38, 283, 555, 649, 18, 82, 13, 1721, 282, 9, 132, 18, 82, 41, 30, 385, 21, 4, 169, 76, 36, 8, 107, 4, 106, 524, 10, 295, 3825, 2, 2476, 6, 3684, 6940, 4, 1126, 41, 263, 84, 395, 649, 18, 82, 838, 1317, 4, 572, 4, 106, 13, 25, 595, 2445, 40, 85, 7369, 518, 5, 4, 1126, 51, 115, 680, 16, 6, 719, 250, 27, 429, 6607, 8, 6940, 114, 343, 84, 142, 20, 5, 1145, 1538, 4, 65, 494, 474, 27, 69, 445, 11, 1816, 6607, 8, 109, 181, 2768, 2, 62, 1810, 6, 624, 901, 6940, 107, 4, 1126, 34, 524, 4, 6940, 1126, 41, 447, 7, 1427, 13, 69, 251, 18, 872, 876, 1539, 468, 9063, 242, 5, 646, 27, 1888, 169, 283, 87, 9, 10, 2, 260, 182, 122, 678, 306, 13, 4, 99, 216, 7, 89, 544, 64, 85, 2333, 6, 195, 7254, 6337, 268, 609, 4, 195, 41, 1017, 2765, 2, 4, 73, 706, 2, 92, 4, 91, 3917, 36, 8, 51, 144, 23, 1858, 129, 564, 13, 269, 678, 115, 55, 866, 189, 814, 604, 838, 117, 380, 595, 951, 320, 4, 398, 57, 2233, 7411, 269, 274, 87, 6607, 8, 787, 283, 34, 596, 661, 5467, 13, 2362, 1816, 90, 2, 84, 22, 2202, 1816, 54, 748, 6607, 8, 87, 62, 6154, 84, 161, 5, 1208, 480, 4, 2, 416, 6, 538, 122, 115, 55, 129, 1104, 1445, 345, 389, 31, 4, 169, 76, 36, 8, 787, 398, 7, 4, 2, 1507, 64, 8862, 22, 125, 2, 9, 2876, 172, 399, 9, 2, 5206, 9, 2, 122, 36, 8, 6642, 172, 247, 100, 97, 6940, 34, 75, 477, 541, 4, 283, 182, 4, 2, 295, 301, 2, 125, 2, 6607, 8, 77, 57, 445, 283, 1998, 217, 31, 380, 704, 51, 77, 2, 509, 5, 476, 9, 2876, 122, 115, 853, 6, 1061, 52, 10, 2, 2, 1308, 5, 4, 283, 182, 36, 8, 5296, 114, 30, 531, 6, 6376, 9, 2470, 529, 13, 2, 2, 58, 529, 7, 2148, 2, 185, 1028, 240, 5296, 1028, 949, 657, 57, 6, 1046, 283, 36, 8, 6607, 8, 4, 2217, 34, 9177, 13, 10, 4910, 5, 4, 141, 283, 120, 50, 2877, 7, 1049, 43, 10, 181, 283, 734, 115, 55, 3356, 476, 6, 2195, 10, 73, 120, 50, 41, 6877, 169, 87, 6607, 8, 107, 144, 23, 129, 120, 169, 87, 33, 2409, 30, 1888, 1171, 161, 4, 294, 517, 23, 2, 25, 398, 9, 2060, 283, 21, 4, 236, 36, 8, 143, 169, 87, 641, 1569, 28, 69, 61, 376, 514, 90, 1249, 62, 2, 13, 4, 2217, 696, 122, 404, 2936, 22, 134, 6, 187, 514, 10, 1249, 107, 4, 96, 1043, 1569, 13, 10, 184, 28, 61, 376, 514, 268, 680, 4, 320, 6, 154, 6, 69, 160, 514, 10, 1249, 27, 4, 153, 5, 52, 29, 36, 8, 6607, 8, 612, 408, 10, 3133, 283, 76, 27, 1504, 31, 169, 951, 2, 122, 36, 8, 283, 236, 62, 641, 84, 618, 2, 22, 8417, 8409, 9, 274, 7322, 399, 7587, 51, 115, 55, 45, 4044, 31, 4, 490, 558, 36, 8, 224, 2, 115, 57, 85, 1655, 2671, 5, 283, 6, 4, 37, 38, 7, 1797, 185, 77, 4446, 4, 555, 298, 77, 240, 2, 7, 327, 652, 194, 8773, 6233, 34, 2, 5463, 4884, 1297, 6, 240, 260, 458, 87, 6, 134, 514, 10, 1249, 22, 196, 514, 4, 37, 38, 309, 213, 54, 207, 8577, 25, 134, 139, 89, 283, 494, 555, 22, 4, 2217, 6, 2172, 4278, 434, 835, 22, 3598, 3746, 434, 835, 7, 48, 6607, 8, 618, 225, 586, 333, 122, 572, 126, 2768, 1998, 62, 133, 6, 2458, 233, 28, 602, 188, 5, 4, 704, 1998, 62, 45, 885, 281, 4, 48, 193, 760, 36, 8, 115, 680, 78, 58, 109, 95, 6, 1732, 1516, 281, 4, 225, 760, 17, 12]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,2):\n",
    "    print(test_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2246"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mdbl', 10996),\n",
       " ('fawc', 16260),\n",
       " ('degussa', 12089),\n",
       " ('woods', 8803),\n",
       " ('hanging', 13796),\n",
       " ('localized', 20672),\n",
       " ('sation', 20673),\n",
       " ('chanthaburi', 20675),\n",
       " ('refunding', 10997),\n",
       " ('hermann', 8804)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting word index dictionary\n",
    "word_index = reuters.get_word_index()\n",
    "word_index_example = take(10, word_index.items())\n",
    "word_index_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10996, 'mdbl'),\n",
       " (16260, 'fawc'),\n",
       " (12089, 'degussa'),\n",
       " (8803, 'woods'),\n",
       " (13796, 'hanging'),\n",
       " (20672, 'localized'),\n",
       " (20673, 'sation'),\n",
       " (20675, 'chanthaburi'),\n",
       " (10997, 'refunding'),\n",
       " (8804, 'hermann')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reversing word index dictionary\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "reverse_word_index_example  = take(10, reverse_word_index.items())\n",
    "reverse_word_index_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10996</td>\n",
       "      <td>mdbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16260</td>\n",
       "      <td>fawc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12089</td>\n",
       "      <td>degussa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8803</td>\n",
       "      <td>woods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13796</td>\n",
       "      <td>hanging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30974</th>\n",
       "      <td>16258</td>\n",
       "      <td>rotting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30975</th>\n",
       "      <td>10995</td>\n",
       "      <td>pods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30976</th>\n",
       "      <td>2849</td>\n",
       "      <td>emery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30977</th>\n",
       "      <td>30979</td>\n",
       "      <td>northerly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30978</th>\n",
       "      <td>16259</td>\n",
       "      <td>onomichi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30979 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1\n",
       "0      10996       mdbl\n",
       "1      16260       fawc\n",
       "2      12089    degussa\n",
       "3       8803      woods\n",
       "4      13796    hanging\n",
       "...      ...        ...\n",
       "30974  16258    rotting\n",
       "30975  10995       pods\n",
       "30976   2849      emery\n",
       "30977  30979  northerly\n",
       "30978  16259   onomichi\n",
       "\n",
       "[30979 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating Word dictionary\n",
    "word_dictionary = pd.DataFrame(reverse_word_index.items())\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     display(word_dictionary)\n",
    "display(word_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? philippine sugar production in the 1987 88 crop year ending august has been set at 1 6 mln tonnes up from a provisional 1 3 mln tonnes this year sugar regulatory administration ? chairman ? yulo said yulo told reuters a survey during the current milling season which ends next month showed the 1986 87 estimate would almost certainly be met he said at least 1 2 mln tonnes of the 1987 88 crop would be earmarked for domestic consumption yulo said about 130 000 tonnes would be set aside for the u s sugar quota 150 000 tonnes for strategic reserves and 50 000 tonnes would be sold on the world market he said if the government approved a long standing ? recommendation to manufacture ethanol the project would take up another 150 000 tonnes slightly raising the target the government for its own reasons has been delaying approval of the project but we expect it to come through by july yulo said ethanol could make up five pct of gasoline cutting the oil import bill by about 300 mln pesos yulo said three major philippine ? were ready to start manufacturing ethanol if the project was approved the ethanol project would result in employment for about 100 000 people sharply reducing those thrown out of work by depressed world sugar prices and a ? domestic industry production quotas set for the first time in 1987 88 had been submitted to president corazon aquino i think the president would rather wait ? the new congress ? after the may elections he said but there is really no need for such quotas we are right now producing just slightly over our own consumption level the producers have never enjoyed such high prices yulo said adding sugar was currently selling locally for 320 pesos per ? up from 190 pesos last august yulo said prices were driven up because of speculation following the ? bid to control production we are no longer concerned so much with the world market he said adding producers in the ? region had learned from their ? and diversified into corn and ? farming and ? production he said diversification into products other than ethanol was also possible within the sugar industry the ? long ago ? their ? yulo said they have 300 sugar mills compared with our 41 but they ? many of them and diversified production we want to call this a ? ? instead of the sugar industry he said sugarcane could be fed to pigs and livestock used for ? ? or used in room ? when you cut sugarcane you don't even have to produce sugar he said yulo said the philippines was lobbying for a renewal of the international sugar agreement which expired in 1984 as a major sugar producer we are urging them to write a new agreement which would revive world prices yulo said if there is no agreement world prices will always be depressed particularly because the european community is ? its producers and dumping sugar on the markets he said current world prices holding steady at about 7 60 cents per pound were ? for the philippines where production costs ranged from 12 to 14 cents a pound if the price holds steady for a while at 7 60 cents i expect the level to rise to about 11 cents a pound by the end of this year he said yulo said economists forecast a bullish sugar market by 1990 with world consumption ? production he said sugar markets were holding up despite ? from artificial sweeteners and high fructose corn syrup but we are not happy with the reagan administration he said since ? we have been regular suppliers of sugar to the u s in 1982 when they restored the quota system they cut ? in half without any justification manila was ? watching washington's moves to cut domestic support prices to 12 cents a pound from 18 cents the u s agriculture department last december slashed its 12 month 1987 sugar import quota from the philippines to 143 780 short tons from 231 660 short tons in 1986 yulo said despite next year's increased production target some philippine mills were expected to shut down at least four of the 41 mills were not working during the 1986 87 season he said we expect two or three more to follow suit during the next season reuter 3\n"
     ]
    }
   ],
   "source": [
    "# Decoding train data message\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in test_data[1]])\n",
    "print(decoded_newswire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8982"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize train and test data  \n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2246"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize train and test labels\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)\n",
    "\n",
    "len(one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Building Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 4s 477us/step - loss: 0.1120 - accuracy: 0.9580 - val_loss: 1.0782 - val_accuracy: 0.8040\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 124us/step - loss: 0.1111 - accuracy: 0.9572 - val_loss: 1.1617 - val_accuracy: 0.7930\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 122us/step - loss: 0.0990 - accuracy: 0.9578 - val_loss: 1.0891 - val_accuracy: 0.8060\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 119us/step - loss: 0.1022 - accuracy: 0.9597 - val_loss: 1.1462 - val_accuracy: 0.7910\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 132us/step - loss: 0.1045 - accuracy: 0.9585 - val_loss: 1.1037 - val_accuracy: 0.8090\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 128us/step - loss: 0.0992 - accuracy: 0.9595 - val_loss: 1.1588 - val_accuracy: 0.8040\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 118us/step - loss: 0.1003 - accuracy: 0.9575 - val_loss: 1.1118 - val_accuracy: 0.8090\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 116us/step - loss: 0.0937 - accuracy: 0.9597 - val_loss: 1.1478 - val_accuracy: 0.8070\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 116us/step - loss: 0.0978 - accuracy: 0.9595 - val_loss: 1.1967 - val_accuracy: 0.7880\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 121us/step - loss: 0.0982 - accuracy: 0.9592 - val_loss: 1.1777 - val_accuracy: 0.7970\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 114us/step - loss: 0.0901 - accuracy: 0.9598 - val_loss: 1.2103 - val_accuracy: 0.7910\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 115us/step - loss: 0.0941 - accuracy: 0.9603 - val_loss: 1.1862 - val_accuracy: 0.8000\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 119us/step - loss: 0.0929 - accuracy: 0.9589 - val_loss: 1.2094 - val_accuracy: 0.7950\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 118us/step - loss: 0.0948 - accuracy: 0.9579 - val_loss: 1.2954 - val_accuracy: 0.7840\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 116us/step - loss: 0.0902 - accuracy: 0.9593 - val_loss: 1.2701 - val_accuracy: 0.7880\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 122us/step - loss: 0.0940 - accuracy: 0.9595 - val_loss: 1.2331 - val_accuracy: 0.7960\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 116us/step - loss: 0.0911 - accuracy: 0.9584 - val_loss: 1.2354 - val_accuracy: 0.7910\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 119us/step - loss: 0.0890 - accuracy: 0.9595 - val_loss: 1.2701 - val_accuracy: 0.7880\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 120us/step - loss: 0.0914 - accuracy: 0.9565 - val_loss: 1.2771 - val_accuracy: 0.7810\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 117us/step - loss: 0.0918 - accuracy: 0.9588 - val_loss: 1.3528 - val_accuracy: 0.7810\n"
     ]
    }
   ],
   "source": [
    "# To train a model we need partial training data and validation data\n",
    "# All of these data are taken from our dataset above\n",
    "# 1000 items for validation and the rest for partial training\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
